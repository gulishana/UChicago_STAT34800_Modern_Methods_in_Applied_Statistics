---
title: "EM_Gaussian Mixture"
output: html_notebook
---

```{r}
# simulated data

# mixture components
mu.true    = c(5, 10)
sigma.true = c(1.5, 2)

# determine Z_i
Z = rbinom(500, 1, 0.75)

# sample from mixture model
X <- rnorm(10000, mean=mu.true[Z+1], sd=sigma.true[Z+1])
hist(X,breaks=15)
```

```{r}
# mixture components are fully specified here
# for each sample Xi we can compute the likelihood P(Xi|Zi=0) and P(Xi|Zi=1)

L = matrix(NA, nrow=length(X), ncol= 2)
L[, 1] = dnorm(X, mean=mu.true[1], sd = sigma.true[1])
L[, 2] = dnorm(X, mean=mu.true[2], sd = sigma.true[2])
```


```{r}
# compute the incomplete data log-likelihood (assuming parameters are known)
# this will be used to determine convergence
compute.log.lik = function(X, L, w) {
  L[,1] = L[,1]*w[1]
  L[,2] = L[,2]*w[2]
  return(sum(log(rowSums(L))))
}


# the driver which checks for convergence by computing 
# the incomplete data log-likelihoods at each step
mixture.EM = function(w.init, L, X) {
  
  w.curr = w.init
  
  # store log-likehoods for each iteration
  log_liks = c()
  ll       = compute.log.lik(X, L, w.curr)
  log_liks = c(log_liks, ll)
  
  delta.ll = 1
  while(delta.ll > 1e-5) {
    w.curr   = EM.iter(w.curr, L)
    ll       = compute.log.lik(X, L, w.curr)
    log_liks = c(log_liks, ll)
    delta.ll = log_liks[length(log_liks)]  - log_liks[length(log_liks)-1]
  }
  
  return(list(w.curr, log_liks))
}


# implement the E and M steps
EM.iter = function(w.curr, L, ...) {
  
  # E-step: compute E_{Z|X,w0}[I(Z_i = k)]
  z_ik = L
  for(i in seq_len(ncol(L))) {
    z_ik[,i] = w.curr[i]*z_ik[,i]
  }
  z_ik = z_ik / rowSums(z_ik)
  
  # M-step
  w.next   = colSums(z_ik)/sum(z_ik)
  return(w.next)
}
```


```{r}
#perform EM
ee = mixture.EM(w.init=c(0.5,0.5), L, X)
print(paste("Estimate = (", round(ee[[1]][1],2), ",", round(ee[[1]][2],2), ")", sep=""))

# inspect the evolution of the log-likelihood and note that it is strictly increases
plot(ee[[2]], ylab='incomplete log-likelihood', xlab='iteration')
```








